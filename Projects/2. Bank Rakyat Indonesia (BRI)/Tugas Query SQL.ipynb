{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BRI-SQL.ipynb","provenance":[{"file_id":"1vJhk8HKULUrzfxv1jQn-nAftwqte1Ppz","timestamp":1630343096033}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mmYRSQMxBlkM"},"source":["**TUGAS FILTERING DATA DI HUE (Mba Arniz)**"]},{"cell_type":"code","metadata":{"id":"WNJTxjLRB1DK"},"source":["SELECT cifno AS CIF, acctno AS \"nomor rekening\",branch ,cbal_base ,`class` AS \"tipe nasabah\" ,group_type ,ds ,status ,open_date \n","FROM datalake.asrs_fact_savingmaster\n","WHERE group_type='Britama' AND status=1 AND ds='202102' LIMIT 100;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nzKKMZqB2Rn"},"source":["**TUGAS FILTERING DATA DI DSW (Mba Ratih)**"]},{"cell_type":"code","metadata":{"id":"F01YttDXB-9o"},"source":["from pyspark.sql import SparkSession, SQLContext, HiveContext\n","from pyspark.conf import SparkConf\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from calendar import monthrange\n","import datetime as dt\n","import pandas as pd\n","from dateutil.relativedelta import relativedelta\n","from pyspark.sql import SQLContext\n","\n","spark = SparkSession.builder.appName(\n","  \"Arniz - ETL Product Recommendation\"\n",").config(\n","  \"spark.dynamicAllocation.enabled\", \"false\"\n",").config(\n","  \"spark.executor.instances\", \"2\"\n",").config(\n","  \"spark.executor.cores\", \"2\"\n",").config(\n","  \"spark.executor.memory\", \"4g\"\n",").config(\n","  \"spark.network.timeout\", 60\n",").config(\n","  \"spark.yarn.executor.memoryOverhead\", \"4g\"\n",").config(\n","  \"spark.driver.maxResultSize\", \"2g\"\n",").enableHiveSupport(\n",").getOrCreate(\n",")\n","\n","sqlContext = SQLContext(spark)\n","pd.options.display.html.table_schema = True\n","pd.options.display.max_rows = 999\n","\n","df= spark.sql(\"\"\"\n","SELECT a.cifno AS cif ,a.acctno AS \"nomor rekening\", a.branch, a.cbal_base,a.`class` AS \"tipe nasabah\", a.group_type,a.ds,a.status,a.open_date, \n","b.tipe_nasabah_desc, b.nama_lengkap, b.jenis_kelamin,b.status_nikah,b.npwp,\n","b.kecamatan_id,b.kelurahan_id,b.handphone,b.pendidikan,b.jenis_pekerjaan,b.nama_kantor,\n","b.kode_bidang_pekerjaan,b.bidang_pekerjaan,b.kode_jabatan,b.jabatan,b.penghasilan_per_bulan,\n","b.omset_per_bulan,b.sumber_penghasilan,b.tujuan_buka_rekening,b.kode_tujuan_buka_rekening, \n","CAST(DATEDIFF(now(), FROM_UNIXTIME(UNIX_TIMESTAMP(b.tanggal_lahir , 'dd-MM-yyyy'))) /365.25 AS INT) AS usia,\n","CONCAT_WS(',',trim(b.alamat_id1),trim(b.alamat_id2),trim(b.alamat_id3),trim(b.alamat_id4)) AS alamat\n","FROM datalake.asrs_fact_savingmaster AS a\n","LEFT JOIN datalake.`6969_crm_dim_dly_customer_info` AS b\n","ON a.cifno=b.`cfcif#`\n","WHERE a.group_type='Britama' AND a.status=1 AND a.ds='202102' AND a.`class`='I' \n","AND year(a.open_date)= 2020\n","\"\"\")\n","df5.show(5)\n","df.write.mode('overwrite').format('parquet').saveAsTable('temp.tabel_simpanan_midah')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Hkb_U1Yaiuo"},"source":["**TUGAS OPTIMALISASI QUERY (Mba Rizka Azmira)**"]},{"cell_type":"code","metadata":{"id":"CxqZV817bB7V"},"source":["#=========================================# \n","#           Import Dependencies           #\n","#=========================================#\n","\n","from pyspark.sql import SparkSession, functions as F, Window\n","from pyspark.sql.types import *\n","from dateutil.relativedelta import relativedelta\n","import datetime\n","from datetime import *\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","#import seaborn as sns\n","import numpy as np\n","from time import sleep\n","\n","#=========================================#\n","#              Pandas Config              #\n","#=========================================#\n","\n","pd.options.display.max_columns = 999\n","pd.options.display.max_rows = 999\n","pd.options.display.html.table_schema = True\n","\n","#=========================================# \n","#              Spark Session              #\n","#=========================================#\n","\n","spark = SparkSession\\\n","  .builder\\\n","  .appName(\"Nanda - Credit Debit Ceria\")\\\n","  .config('spark.dynamicAllocation.enabled', 'false')\\\n","  .config('spark.executor.instances', '6')\\\n","  .config('spark.executor.cores', '5')\\\n","  .config('spark.executor.memory', '10g')\\\n","  .config('spark.yarn.executor.memoryOverhead', '8g')\\\n","  .enableHiveSupport()\\\n","  .getOrCreate()\n","  \n","#algoritma\n","#1. ambil whitelist_account_number menjadi acctno from datamart.ceria_Master_customer\n","#2. ngambil beberapa fitur dari datalake.`8000_rc_trx_as4_ddhist by join acctno\n","#3. ddhist difilter 2020 dan 2021 dan month < 4\n","#4. dari langkah 2 dan 3, menghitung transaksi keluar (Debit) dan transaksi masuk (Credit) dalam satu bulan\n","\n","\n","spark.sql('drop table if exists temp.try_midah_ceria3')\n","\n","list_ds = ['202001', '202002','202003','202004','202005','202006','202007','202008','202009','202010','202011','202012',\n","           '202101','202102','202103','202104']\n","\n","for i in range(16):\n","  ds = list_ds[i]\n","  print(ds)\n","  \n","  #mengabil whitelist acc\n","  acc_whitelist=spark.sql(\"\"\"\n","  select CAST(whitelist_account_number AS DECIMAL(19,0)) AS acctno \n","  FROM datamart.ceria_Master_customer\n","  \"\"\")\n","  acc_whitelist.createOrReplaceTempView('acc_whitelist')\n","\n","  #filter ddhist\n","  df_ddhist=spark.read.table('datalake.`8000_rc_trx_as4_ddhist`')\\\n","                .filter(\"concat(year,lpad(month,2,'0')) like '{}%'\".format(ds))\n","  df_ddhist.createOrReplaceTempView('df_ddhist')\n","\n","  #join untuk mengambil fitur di ddmast\n","  fitur_whitelist=spark.sql(\"\"\"\n","  SELECT DISTINCT tracct,trdorc,trdate,\n","                      CONCAT(year, '-', month, '-', day) AS dt,\n","                      trtime,trremk,seq,amt\n","  FROM acc_whitelist a\n","  LEFT JOIN df_ddhist b\n","  ON trim(a.acctno) = trim(b.tracct)\"\"\")\n","  fitur_whitelist.createOrReplaceTempView('df_whitelist_features')\n","\n","  table_final = spark.sql(\"\"\"\n","  SELECT tracct,TRUNC(dt, 'month') AS month_dt,\n","             SUM(CASE WHEN trdorc = 'D' THEN amt END) AS debit_amt,\n","             SUM(CASE WHEN trdorc = 'C' THEN amt END) AS credit_amt,\n","             SUM(CASE WHEN trdorc = 'C' THEN amt ELSE 0 END) - SUM(CASE WHEN trdorc = 'D' THEN amt ELSE 0 END) AS disp_income_amt \n","  FROM df_whitelist_features\n","  GROUP BY tracct,TRUNC(dt, 'month')\"\"\")\n","\n","  #write to HDFS\n","  table_final\\\n","    .write\\\n","    .mode('append')\\\n","    .format('parquet')\\\n","    .saveAsTable(\"temp.try_midah_ceria3\")\\\n","\n","  spark.sql('refresh table temp.try_midah_ceria3')  \n","  table_final.show(1000)"],"execution_count":null,"outputs":[]}]}